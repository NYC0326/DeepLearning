import os
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
import requests
import re
import pandas as pd
from langchain_community.document_loaders import PyPDFLoader
from sklearn.metrics.pairwise import cosine_similarity
from nltk.tokenize import sent_tokenize
import nltk
nltk.download('punkt_tab')
from tqdm import tqdm
from langchain_core.documents import Document

# ====== ÏÑ§Ï†ï ======
os.environ["GOOGLE_API_KEY"] = os.getenv('GOOGLE_API_KEY')
PDF_DIR = "pdfs"
VECTOR_DIR = "vectorstores/on_demand"
THRESHOLD = 0.75

# ====== Î™®Îç∏ Ï§ÄÎπÑ ======
embedding_model = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
llm = ChatGoogleGenerativeAI(model="models/gemini-1.5-flash", temperature=0.4, max_output_tokens=1536)

# ====== Ïú†Ìã∏ Ìï®Ïàò ======
def clean_text(text):
    text = re.sub(r'-\n', '', text)
    text = re.sub(r'\n', ' ', text)
    return re.sub(r'\s+', ' ', text).strip()

def semantic_chunk(text, threshold=THRESHOLD):
    sentences = sent_tokenize(text)
    if len(sentences) < 2:
        return [text]
    embeddings = embedding_model.embed_documents(sentences)
    chunks, chunk = [], [sentences[0]]
    for i in range(1, len(sentences)):
        sim = cosine_similarity([embeddings[i - 1]], [embeddings[i]])[0][0]
        if sim < threshold:
            chunks.append(" ".join(chunk))
            chunk = []
        chunk.append(sentences[i])
    if chunk:
        chunks.append(" ".join(chunk))
    return chunks

def process_pdf(file_path, paper_title=""):
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    raw_text = "\n".join([d.page_content for d in docs])
    cleaned = clean_text(raw_text)
    chunks = semantic_chunk(cleaned)
    if paper_title:
        chunks.insert(0, f"ÎÖºÎ¨∏ Ï†úÎ™©: {paper_title}")
    return chunks

# ====== Streamlit UI ======
st.set_page_config(page_title="ÎÖºÎ¨∏ Í≤ÄÏÉâ Î∞è ÏßàÏùòÏùëÎãµ", layout="wide")
st.title("üìö ÎÖºÎ¨∏ Í≤ÄÏÉâ + PDF RAG")

query = st.text_input("üîç Ï∞æÍ≥† Ïã∂ÏùÄ ÎÖºÎ¨∏ Ï£ºÏ†úÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî:")

# ---- Abstract Í≤ÄÏÉâ ----
if query:
    st.subheader("üîé Í¥ÄÎ†® ÎÖºÎ¨∏ Í≤ÄÏÉâ Í≤∞Í≥º")
    abstracts_store = FAISS.load_local("vectorstores/abstracts_faiss", embedding_model, allow_dangerous_deserialization=True)
    docs = abstracts_store.similarity_search(query, k=5)

    for i, doc in enumerate(docs):
        st.markdown(f"**{i+1}. Title**: {doc.page_content.split('\n')[0].replace('Title: ', '')}")
        st.markdown(f"*Abstract*: {doc.page_content.split('\n')[1].replace('Abstract: ', '')}")
        st.markdown(f"`arXiv ID`: {doc.metadata['source']}")
        st.markdown("---")

# ---- PDF ID ÏûÖÎ†• Î∞è ÏûêÎèô Ï≤òÎ¶¨ ----
st.subheader("üìÑ arXiv ÎÖºÎ¨∏ ID ÏûÖÎ†• ‚Üí PDF Ï≤òÎ¶¨")
id_input = st.text_input("arXiv IDÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî (Ïòà: 1404.0736):")
process_button = st.button("ÎÖºÎ¨∏ Ï≤òÎ¶¨ÌïòÍ∏∞")

if process_button and id_input:
    with st.spinner("PDF Îã§Ïö¥Î°úÎìú Ï§ë..."):
        pdf_url = f"https://arxiv.org/pdf/{id_input}.pdf"
        response = requests.get(pdf_url)
        if response.status_code != 200:
            st.error("‚ùå PDF Îã§Ïö¥Î°úÎìú Ïã§Ìå®!")
        else:
            os.makedirs(PDF_DIR, exist_ok=True)
            file_path = os.path.join(PDF_DIR, f"{id_input.replace('/', '_')}.pdf")
            with open(file_path, "wb") as f:
                f.write(response.content)

            # metadataÏóêÏÑú ÎÖºÎ¨∏ Ï†úÎ™© Í∞ÄÏ†∏Ïò§Í∏∞
            title = ""
            try:
                meta = pd.read_csv("metadata/abstracts.csv")
                row = meta[meta["id"] == id_input]
                if not row.empty:
                    title = row["title"].values[0]
            except:
                pass

            with st.spinner("Ï≤≠ÌÇπ Î∞è Î≤°ÌÑ∞ DB Ï†ÄÏû• Ï§ë..."):
                chunks = process_pdf(file_path, paper_title=title)
                documents = [Document(page_content=chunk, metadata={"source": id_input}) for chunk in chunks]
                faiss_store = FAISS.from_documents(documents, embedding_model)
                faiss_store.save_local(VECTOR_DIR)
                st.success("‚úÖ Ï≤òÎ¶¨ ÏôÑÎ£å! ÏßàÎ¨∏Ìï¥Î≥¥ÏÑ∏Ïöî.")

# ---- ÏßàÎ¨∏ UI ----
st.subheader("ü§ñ ÎÖºÎ¨∏ ÎÇ¥Ïö© ÏßàÎ¨∏ÌïòÍ∏∞")
user_q = st.text_input("ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî:")
if user_q:
    try:
        local_store = FAISS.load_local(VECTOR_DIR, embedding_model, allow_dangerous_deserialization=True)
        qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=local_store.as_retriever())
        response = qa_chain.invoke(user_q)
        st.markdown(response['result'])
    except:
        st.error("Î≤°ÌÑ∞ DBÍ∞Ä ÏïÑÏßÅ ÏóÜÏñ¥Ïöî! Î®ºÏ†Ä ÎÖºÎ¨∏ PDFÎ•º ÏûÖÎ†•Ìï¥ Ï£ºÏÑ∏Ïöî.")
